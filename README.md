# Fast question-answering on Covid-19 dataset using BERT style models finetuned on Squad 2

**Covid-19 Fast QA** is an interactive experimental tool leveraging a state-of-the-art language model to search relevant content inside the [COVID-19 Open Research Dataset (CORD-19)](https://pages.semanticscholar.org/coronavirus-research) recently published by the White House and its research partners. The dataset contains over 100,000 scholarly articles about COVID-19, SARS-CoV-2 and related coronaviruses.

The system uses [`bert-large-uncased-whole-word-masking-finetuned-squad`](https://huggingface.co/transformers/pretrained_models) and my own `bert-base-uncased-finetuned-squad` models to perform fast question-answering on documents in the Covid-19 dataset. Fast inference is achieved by tokenizing the document content during pre-processing and then adjusting the tok-to-orig indices based on the number of tokens in the query. This results in a 5x speed up over the default transformers implementation which performs tokenization for every query. 

A screenshot of the system frontend (query results are marked in yellow) are shown below. 

![demo1](img/demo1.png)


## Setup

Tested on: Ubuntu 18.04, with 2 1080Ti GPU. Docker version 19.03.5

First clone the repo. Then, [download](https://drive.google.com/drive/folders/1w0TjWZA_vpxVewkHUTiSyVS9Exz-g1VW) pre-processed data and BERT models. The data is structured like this:

* /data/cord19q folder contains the output of the cord19q ETL process and the original covid19 documents
* /data/cord19q/current/document_parses: contents of the document_parses folder that you should download from [here](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)
* /data/cord19q/models: contains the bert-large and bert-small models finetuned on Squad 2 QA dataset. If you have trouble downloading the models (sometimes Google Drive hangs while trying to perform a virus scan on large binary files), browse into the individual model directories on Google drive, download vocab.txt, config.json and pytorch_model.bin, create the directories for the two models manually on your host machine and copy over vocab.txt, config.json and pytorch_model.bin to the respective model directories. For bert-base-uncased_finetuned_squad model, your directory structure should like this:

```angular2
/data/models/bert-base-uncased_finetuned_squad model
   -vocab.txt
   -pytorch_model.bin
   -config.json
```
Similarly for bert-large-uncased-whole-word-masking-finetuned-squad.

The core application functionality is exposed via the cord19q_lookup (terrible name, I know.., see src/covid_browser/modelapi.py for implementation) endpoint that receives a user query (eg: "where did covid-19 originate"), and returns back top-n matching sentences, information about the corresponding article such as title, data published, URL etc and relevant excerpts. The excerpts are generated by applying a BERT model fine-tuned on the SQuAD QA dataset. 

The application server can be run directly via Python or as a docker container. It expects three environment variables - BASE_PATH (the base application directory), USE_GPU (set to 1 if your computer has a supported GPU - eg. Nvidia 1080Ti, 0 otherwise) and PORT (the port number Flask listens on). See *run_application_server* in *kube/ctnr_mgmt.sh* for how to run *docker build* and *docker run* commands. Note the docker volume mapping and port mapping options. You should change the host paths to the correct paths on your system.

The application Docker container is built using the Dockerfile in the main directory. This Dockerfile does the following: 
* Install Ubuntu 18.04 with CUDA 10.1
* Install Python3.7 and Pip
* Install Pytorch 1.5 with CUDA support 
* Install other dependencies (see requirements.txt) - Flask, Transformers etc.
* Copy parent directory to /app in Docker container (except the contents of the data directory, see .dockerignore, to keep the size of the image small). We'll map the data directory on the host to the container when we run the container.
* Start a Flask server on 0.0.0.0/PORT (PORT number is specified as an env variable)

The application container logs are mapped to log/logs.txt

## GPU Support
To enable GPU acceleration within the Docker container, follow these [instructions](https://github.com/NVIDIA/nvidia-docker). 

```angular2
# Add the package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

To run with GPU support, set the `USE_GPU` environment variable to 1. See *run_application_server kube/ctnr_mgmt.sh()

## Application Frontend
To application front-end is implemented in */apache* and served by an Apache server running as a docker container. See *run_apache_proxy_server* in *kube/ctnr_mgmt.sh* for how to run *docker build* and *docker run* commands. The *ctnr_mgmt.sh* script runs two Apache servers on ports 8000 and 8082. For a local deployment, you only need the Apache server running on port 8082. This server serves HTML/js content located in *apache/dev/static* and proxies the traffic at endpoints /cord19q_lookup, /healthcheck and /stats (see apache/dev/covid-demo.conf) to the application server. Note that the IP address of the docker bridge (172.17.0.1) is used in the proxy settings. If you are running the application server directly (eg. in a Python IDE, or using the command line) then this should be replaced by the localhost address (127.0.0.1)

Once the application and Apache server containers are running, open a browser and type:http://localhost:8082/covid-demo/main.html. You should see the application frontend (see screenshot above).

## Code structure
The code is structured as follows:
* The *apache* directory contains the javascript, html, css and apache server config files. There is a development version in the dev folder and a "prod" version in the sites folder. For local deployment, you only need the dev version. The two versions share the apache Dockerfile and config (apache/httpd.config). The Dockerfile builds the Apache container image which is used to run two local Apache servers on different ports - 8000 and 8002 (see kube/ctnr_mgmt.sh). Since you'll be running the application locally only, you only need the dev apache server. 

* *data/model* contains the BERT large and small models
* *data/cord19q* contains the Covid-19 research dataset and output of the cord19q ETL process
* *src/cord19q* contains the cord19q ETL code
* *src/covid_browser* contains the application code, BERT lookup etc. 
